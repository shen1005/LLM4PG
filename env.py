import gymnasium as gym
import numpy as np
from gymnasium.core import RenderFrame
from enum import Enum, unique
import torch
from Reward import RewardPredictor


@unique
class Act(Enum):
    left = 0
    right = 1
    forward = 2
    pickup = 3
    drop = 4
    toggle = 5
    done = 6

@unique
class Obj(Enum):
    unseen = 0
    empty = 1
    wall = 2
    floor = 3
    door = 4
    key = 5
    ball = 6
    box = 7
    goal = 8
    lava = 9
    agent = 10

# ç›¸å¯¹äºŽlavaæ‰€é€ æˆçš„å¤¹è§’
@unique
class Direction(Enum):
    up = 0
    right = 1
    down = 2
    left = 3

@unique
class env_type(Enum):
    lava = 'MiniGrid-LavaGapS7-v0'
    cross = 'MiniGrid-Unlock-v0'

    '''è¿™é‡Œçš„æƒ…å†µæ¯”è¾ƒå¤æ‚ã€‚é¦–å…ˆåˆ†å‡ æ–¹é¢è€ƒè™‘ï¼š
    1.å¦‚æžœå·¦å‰æ–¹å’Œå³å‰æ–¹éƒ½æ˜¯å¢™ï¼Œé¦–å…ˆç¡®å®šçš„æ˜¯ä¸€å®šåœ¨å¢™é™„è¿‘ã€‚é‚£ä¹ˆå¯ä»¥åˆ†æˆé¢æœå¢™å’Œä¾§å¯¹å¢™ã€‚ä¾§å¯¹å¢™ä¸€å®šæ— æ³•è¾¾æˆè¿™ç±»æƒ…å†µï¼Œå¿…ç„¶æ˜¯é¢å¯¹å¢™ï¼Œè€Œé¢å¯¹å¢™å¿…ç„¶å‰é¢ä¸æ˜¯ç©ºç™½
    2.å¦‚æžœå·¦å‰æ–¹å³å‰æ–¹éƒ½æ˜¯å²©æµ†ï¼Œä¾§å¯¹å²©æµ†ä¸€å®šä¸å¯èƒ½ï¼Œå› ä¸ºå²©æµ†åªæœ‰ä¸€æ¡ï¼Œåªèƒ½é¢å¯¹å²©æµ†ï¼Œè€Œé¢å¯¹å²©æµ†åˆ™ä¸€å®šä¼šæœ‰å‰é¢ä¸æ˜¯ç©ºç™½
    3.å¦‚æžœä¸€è¾¹æ˜¯å¢™ï¼Œä¸€è¾¹æ˜¯å²©æµ†åˆ™æ¯”è¾ƒæ£˜æ‰‹ã€‚è¿™ç§æƒ…å½¢ä¸€å®šæ˜¯åŒæ—¶ä¾§å¯¹å²©æµ†å’Œå¢™ã€‚å…¶ä¸­ï¼Œè¿™å‡ ç§æƒ…å½¢å¾ˆæ£˜æ‰‹ï¼š
    ##############
    #ðŸ”º          #
    #   ~~~~~~~~~#          
    #            #
    #            #
    #            #
    ##############
    æ‰€ä»¥ç›®å‰çš„æƒ³æ³•æ˜¯ï¼Œé™¤äº†é™åˆ¶æ¡ä»¶æ˜¯å·¦å‰å³å‰éƒ½æ˜¯å²©æµ†ï¼Œå‰æ–¹ç©ºç™½ä¹‹å¤–ï¼Œè¿˜è¦æ·»åŠ çº¦æŸï¼š
    å·¦å‰å³å‰çš„ä»»æ„ä¸€ä¸ªå²©æµ†ä»–çš„å‰å’ŒåŽéƒ½æ˜¯ç©ºç™½
    ä¸ºä»€ä¹ˆè¯´å®ƒæ˜¯å¯¹çš„ï¼Ÿä¸»è¦ä»Žä¸¤ä¸ªè§’åº¦æ¥ç¡®è®¤ï¼šå³ä¸æ¼ä¸é”™
    ä¸æ¼ï¼šé¦–å…ˆåœ¨æ­£ç¡®çš„æœå‘ä¸‹ï¼Œä»–çš„å²©æµ†å‰é¢å¿…ç„¶æ˜¯ç©ºï¼Œä¸å¯èƒ½æ˜¯å¢™ï¼Œå› ä¸ºæ™ºèƒ½ä½“æœ€ç»ˆè¦ç©¿è¿‡åŽ»;å…¶æ¬¡ï¼ŒåŽé¢å¿…ç„¶ä¸èƒ½æ˜¯å¢™ï¼Œå¦åˆ™ä¼šå‡ºçŽ°å¢™ç©¿è¿‡å²©æµ†æˆ–è€…æ™ºèƒ½ä½“åœ¨å¢™å†…æƒ…å†µ
    ä¸é”™ï¼šåœ¨ä¸Šå›¾ç­›é€‰å‡ºçš„é‚£ä¸ªæ¡ä»¶ä¸‹ï¼Œå²©æµ†ä¾§èº«çš„æ—¶å€™æ¼”è®²å¸¦åªæœ‰ä¸€å¤„ç©ºç™½ï¼Œæ²¡æœ‰åˆ«çš„ç©ºç™½ï¼Œä¸å¯èƒ½æ»¡è¶³æˆ‘çš„çº¦æŸæ¡ä»¶ã€‚
    '''

def is_lava_empty(state, idx):
    if state[idx] == Obj.lava.value and state[idx + 1] == Obj.empty.value and state[idx - 1] == Obj.empty.value:
        return True
    return False

def is_lava_gap_towards(state):
    if (state[19] == Obj.lava.value or state[19] == Obj.wall.value) and (state[33] == Obj.lava.value or state[33] == Obj.wall.value) and state[26] == Obj.empty.value and (is_lava_empty(state, 19) or is_lava_empty(state, 33)):
        return True
    return False

# TODO æœ‰bugï¼Œ å¦‚æžœå·¦è¾¹æ˜¯å¢™ï¼Œå³è¾¹æ˜¯é¡ºç€é¢æœæ–¹å‘çš„å²©æµ†ï¼Œè¿™ä¸ªæ—¶å€™ä¹Ÿä¼šè¿”å›žTrue
def is_in_lava_gap(state):
    if (state[20] == Obj.lava.value or state[20] == Obj.wall.value) and (state[34] == Obj.lava.value or state[34] == Obj.wall.value):
        return True
    return False


class CrossEnv(gym.Env):
    def __init__(self, env, use_reward_predictor=False, type="cross", gamma=0.1):
        super(CrossEnv, self).__init__()
        self.mini_env = env
        self.action_space = env.action_space
        min_value = np.min(env.observation_space['image'].low)
        max_value = np.max(env.observation_space['image'].high)
        if type =="cross_3":
            self.observation_space = gym.spaces.Box(low=min_value,
                                                    high=max_value,
                                                    dtype=env.observation_space['image'].dtype,
                                                    shape=(env.observation_space['image'].shape[0]*env.observation_space['image'].shape[1] + 2,))
        else:
            self.observation_space = gym.spaces.Box(low=min_value,
                                                    high=max_value,
                                                    dtype=env.observation_space['image'].dtype,
                                                    shape=(env.observation_space['image'].shape[0]*env.observation_space['image'].shape[1],))
        self.now_state = None
        self.has_key = False            # è¡¨ç¤ºæ™ºèƒ½ä½“æ˜¯ä¸æ˜¯æ‹¿åˆ°äº†é’¥åŒ™
        self.drop_times = 0             # ä¸¢ä¸‹é’¥åŒ™çš„æ¬¡æ•°
        self.stepList = []              # å·²ç»èµ°è¿‡çš„steps
        self.num_steps = 0
        self.actions_dict = {
            'w': self.mini_env.actions.forward,
            'a': self.mini_env.actions.left,
            'd': self.mini_env.actions.right,
            's': self.mini_env.actions.pickup,
            ' ': self.mini_env.actions.toggle,
            'q': self.mini_env.actions.done,
            'e': self.mini_env.actions.drop
        }
        self.use_reward_predictor = use_reward_predictor
        print("use_reward_predictor: ", use_reward_predictor)
        if use_reward_predictor:
            self.reward_model = RewardPredictor(4, 64, 1)
            self.reward_model.load_state_dict(torch.load("reward_predictor.pth"))
            # self.reward_model.load_state_dict(torch.load("reward_predictor_mixtral_unlock.pth"))

        '''for lava env, some parameters are different from cross env'''
        self.env_type = type
        self.direction = Direction.up
        self.lava_state = 0 # åŒºåˆ«äºŽcross envçš„now_state
        self.is_have_cross_lava = False     # æ˜¯å¦å·²ç»ç©¿è¿‡lava
        self.down_times = 0
        self.is_drop_into_lava = False

        self.cost = 0

    def ob2state(self, obs):
        array = obs['image']
        # è¦æŠŠè¿™ä¸ª7*7*3çš„arrayå˜æˆ7*7, åªå–æ¯ä¸ª3çš„ç¬¬ä¸€ä¸ªå€¼
        state = np.array(array[:, :, 0]).flatten()
        # å†åœ¨åŽé¢åŠ ä¸Šé’¥åŒ™çš„çŠ¶æ€
        if self.env_type == "cross_3":
            state = np.append(state, self.drop_times)
            state = np.append(state, self.has_key)
            # if self.drop_times == 3:
                #print("--------get 3 times---------")
        return state

    def reset(self, **kwargs):
        obs = self.mini_env.reset()[0]
        state = self.ob2state(obs)
        self.now_state = state
        self.has_key = False
        self.drop_times = 0
        self.stepList = []
        self.num_steps = 0

        self.direction = Direction.up
        self.lava_state = 0  # åŒºåˆ«äºŽcross envçš„now_state
        self.is_have_cross_lava = False  # æ˜¯å¦å·²ç»ç©¿è¿‡lava
        self.down_times = 0
        self.is_drop_into_lava = False
        state = self.ob2state(obs)
        self.cost = 0
        return state, {}

    def render(self):
        return self.mini_env.render()

    # è¾“å…¥çŽ°åœ¨çš„stateå’Œaction, çœ‹çœ‹æ˜¯ä¸æ˜¯è®©é’¥åŒ™çŠ¶æ€å‘ç”Ÿäº†æ”¹å˜
    def key_change(self, state, action):
        if action == Act.drop.value and self.has_key:
            self.has_key = False
            self.stepList.append(self.num_steps)
            self.num_steps = 0
            self.drop_times += 1
        if action == Act.pickup.value and state[26] == Obj.key.value:
            self.has_key = True
            self.stepList.append(self.num_steps)
            self.num_steps = 0

    def lava_change(self, state, action):
        if is_lava_gap_towards(state) and action == Act.forward.value:
            if self.lava_state != 0:
                print("error")
            assert self.lava_state == 0     # é¢„é˜²ä¸€ä¸‹bug
            self.lava_state = 1
            self.direction = Direction.up.value if self.is_have_cross_lava is False else Direction.down.value
        elif self.lava_state == 1 and action == Act.forward.value:
            '''è¿™é‡Œè¦æ³¨æ„ä¸‡ä¸€æ—è¾¹æ˜¯å¢™ï¼Œå‘å‰èµ°å°±ç›¸å½“äºŽæ²¡åŠ¨'''
            if self.direction == Direction.up.value:
                self.is_have_cross_lava = True
                self.lava_state = 0
            elif self.direction == Direction.down.value:
                self.is_have_cross_lava = False
                self.lava_state = 0
                self.down_times += 1
            else:
                pass
        elif self.lava_state == 1 and action == Act.left.value:
            '''å‘å·¦è½¬ï¼Œ è¿™é‡Œä¹Ÿè¦æ ¹æ®å½“å‰çš„æœå‘è°ƒæ•´æ–¹å‘'''
            if self.direction == Direction.up.value:
                self.direction = Direction.left.value
            elif self.direction == Direction.down.value:
                self.direction = Direction.right.value
            elif self.direction == Direction.left.value:
                self.direction = Direction.down.value
            elif self.direction == Direction.right.value:
                self.direction = Direction.up.value
        elif self.lava_state == 1 and action == Act.right.value:
            '''å‘å³è½¬åŒç†'''
            if self.direction == Direction.up.value:
                self.direction = Direction.right.value
            elif self.direction == Direction.down.value:
                self.direction = Direction.left.value
            elif self.direction == Direction.left.value:
                self.direction = Direction.up.value
            elif self.direction == Direction.right.value:
                self.direction = Direction.down.value
        if state[26] == Obj.lava.value and action == Act.forward.value:
            self.is_drop_into_lava = True


    def step(self, action):
        # æ³¨æ„è¿™é‡Œçš„å‰åŽé¡ºåºï¼špick up çš„æ­¥éª¤ç®—åˆ°äº†æ‹¿å–é’¥åŒ™äº†çš„æ­¥éª¤é‡Œ
        self.num_steps += 1
        key_before = self.has_key
        if self.env_type == "lava":
            self.lava_change(self.now_state, action)
        else:
            self.key_change(self.now_state, action)
        obs, reward, done, info, _ = self.mini_env.step(action)
        state = self.ob2state(obs)
        self.now_state = state
        if done:
            self.stepList.append(self.num_steps)
            self.num_steps = 0
            success = 1 if reward > 0 else 0
            if self.use_reward_predictor and (self.env_type == "cross" or self.env_type == "cross_3"):
                reward = self.reward_model(torch.tensor([success, self.drop_times, sum(self.stepList)], dtype=torch.float32)).item()
                self.cost = (self.drop_times - 3) ** 2
                #reward = reward - 0.2 * self.cost
                # print(f"reward: {reward}")
                # print(f"{self.use_reward_predictor}, {self.env_type}")
                # exit(0)
            elif self.use_reward_predictor and self.env_type == "lava":
                reward = self.reward_model(torch.tensor([success, sum(self.stepList), self.is_have_cross_lava, self.is_drop_into_lava], dtype=torch.float32)).item()
            else:
                #reward = 0 if reward <= 0 else reward
                self.cost = (self.drop_times - 3) ** 2
                # reward = reward - 0.9 * self.cost
                reward = -sum(self.stepList) if reward <= 0 else reward
                # reward = reward
        return state, reward, done, info, {}

    def getNowReward(self):
        return self.reward_model(torch.tensor([0, self.drop_times, sum(self.stepList)], dtype=torch.float32)).item()

    def force_done(self):
        self.stepList.append(self.num_steps)
        self.cost = (self.drop_times - 3) ** 2
        self.num_steps = 0

    def analyse(self):
        step_num = sum(self.stepList)
        return self.drop_times, step_num

    def analyse2lava(self):
        return self.down_times, sum(self.stepList), self.is_have_cross_lava, self.is_drop_into_lava

    def analyse2cost(self):
        return self.cost

def sampleData(env_name='MiniGrid-Unlock-v0', type="cross", times=500):
    render = False
    resultWriter = []
    if render:
        mini_env = gym.make(env_name, render_mode="human")
    else:
        mini_env = gym.make(env_name)
    env = CrossEnv(mini_env, type=type)
    obs, _ = env.reset()
    times = times
    step_num = 0
    while times > 0:
        # action = env.actions_dict[input("è¯·è¾“å…¥åŠ¨ä½œ: ")]
        action = env.action_space.sample()
        # noinspection PyTupleAssignmentBalance
        obs, reward, done, info, _ = env.step(action)
        step_num += 1
        if reward != 0:
            print("reward: ", reward)
            print(action)
            print(env.analyse())
        if render:
            env.render()
        if done or step_num > 500:
            step_num = 0
            env.force_done()
            if reward == 0:
                result = 0
            elif reward > 0:
                result = reward
            else:
                # raise Exception("reward < 0")
                result = reward
            # è½¨è¿¹
            if env.env_type == "lava":
                drop_times, step_nums, is_have_cross_lava, is_drop_into_lava = env.analyse2lava()
                resultWriter.append([result, step_nums, is_have_cross_lava, is_drop_into_lava])
            else:
                drop_times, step_nums = env.analyse()
                resultWriter.append([result, drop_times, step_nums])
            obs = env.reset()[0]
            env.force_done()
            print("done, ------------------------------------")
            times -= 1
            print(f"å‰©ä½™æ¬¡æ•°: {times}")
    # æŽ’åº
    resultWriter.sort(key=lambda x: x[0])
    # å˜æˆnp.array
    resultWriter = np.array(resultWriter)
    # å†™å…¥æ–‡ä»¶
    print(resultWriter)
    with open("result.txt", "w") as f:
        for i in resultWriter:
            if env.env_type == "lava":
                f.write(f"{i[0]} {i[1]} {i[2]} {i[3]}\n")
            else:
                f.write(f"{i[0]} {i[1]} {i[2]}\n")
    # å­˜æˆnpy
    np.save("result.npy", resultWriter)

def operate_by_hand():
    env = gym.make('MiniGrid-Unlock-v0', render_mode="human")
    env = CrossEnv(env)
    obs, _ = env.reset()
    while True:
        action = input("è¯·è¾“å…¥åŠ¨ä½œ: ")
        action = env.actions_dict[action]
        print(action)
        obs, reward, done, info, _ = env.step(action)
        env.render()
        if done:
            print(reward, info)
            break


if __name__ == '__main__':
    env_name = "MiniGrid-Unlock-v0"
    #sampleData(env_name=env_name, type="cross", times=5000)
    data = np.load("result.npy")
    # ç¬¬ä¸€ä¸ªå…ƒç´ å¦‚æžœæ˜¯æ­£æ•°ï¼Œå˜æˆ1ï¼Œå¦‚æžœæ˜¯è´Ÿæ•°ï¼Œå˜æˆ0
    data[:, 0] = np.where(data[:, 0] > 0, 1, 0)
    with open("data.txt", "w") as f:
        for i in data:
            f.write(f"{i[0]} {i[1]} {i[2]}\n")
    np.save("data2.npy", data)





